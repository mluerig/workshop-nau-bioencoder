model:
  backbone: timm_tf_efficientnet_b5.ns_jft_in1k # model architecture and pre-trained weights to use - see https://github.com/agporto/BioEncoder/blob/main/help/05-options.md#models
  num_classes: 12 # Number of output classes for classification

train:
  n_epochs: &epochs 10 # Number of training epochs
  amp: True # Enable Automatic Mixed Precision (AMP) for faster training on compatible GPUs
  ema: True # Use Exponential Moving Average to stabilize training
  ema_decay_per_epoch: 0.4 # EMA decay rate; adjust based on dataset size
  target_metric: accuracy # Metric to optimize during training
  stage: second # Training stage: 'first' for SupCon, 'second' for fine-tuning classification

dataloaders:
  train_batch_size: 120
  valid_batch_size: 120
  num_workers: 4 

optimizer:
  name: SGD # Optimizer type - see https://github.com/agporto/BioEncoder/blob/main/help/05-options.md#optimizers
  params:
    lr: 0.3 # Learning rate

scheduler:
  name: CosineAnnealingLR # Learning rate scheduler - see https://github.com/agporto/BioEncoder/blob/main/help/05-options.md#schedulers
  params:
    T_max: *epochs # Maximum number of iterations
    eta_min: 0.002 # Minimum learning rate

criterion:
  name: 'LabelSmoothing' # Loss function - see https://github.com/agporto/BioEncoder/blob/main/help/05-options.md#losses
  params:
    classes: 100 # Number of classes (adjust based on actual number of classes)
    smoothing: 0.01 # Smoothing factor for label smoothing

img_size: &size 128

augmentations:
  sample_save: True
  sample_n: 10
  sample_seed: 42 
  transforms: 
    - RandomResizedCrop:
        height: *size
        width: *size 
        scale:  !!python/tuple [0.7,1]
    - Flip:
    - RandomRotate90:
    - MedianBlur:
        blur_limit: 3
        p: 0.3
    - ShiftScaleRotate:
        p: 0.4
    - OpticalDistortion:
    - GridDistortion:
    - HueSaturationValue: